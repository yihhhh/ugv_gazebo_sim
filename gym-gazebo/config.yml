env_config:
    stack_obs: True
    action_repeat: 2
    max_episode_length: 100
    reward_distance: 1.0  # reward scale
    reward_angle: 0.3
    reward_goal_met: 2.5
    lvel_lim: 0.3  # linear velocity limit
    rvel_lim: 1.0  # rotational velocity limit

    render: False

    env_0:
        region_bound: 2.5
        collision_region: 0.45
        cost_region: 0.6
        goal_region: 0.3
        robot_pos: [-0.5, -1.5, 0]
        goal_pos: [1.5, 0.5]
        cyls_pos: [[-1.5, -0.5], [-0.5, -0.5], [0.5, -0.5]]
    
    env_1:
        region_bound: 2.5
        collision_region: 0.45
        cost_region: 0.6
        goal_region: 0.3
        robot_pos: [1.5, -1.5, 1.571]
        goal_pos: [-1.5, 1.5]
        cyls_pos: [[1.5, 0.5], [-0.5, 0.5], [-0.5, -1.5]]

    env_2:
        region_bound: 2.5
        collision_region: 0.45
        cost_region: 0.6
        goal_region: 0.3
        robot_pos: [1.5, 1.5, 3.14]
        goal_pos: [1.5, -1.5]
        cyls_pos: [[1.5, 0.5], [-0.5, 0.5], [0.5, -0.5], [-1.5, -1.5]]

# MPC controller configuration
mpc_config:
    horizon: 6 # how long of the horizon to predict
    gamma: 0.98              # reward discount coefficient
    RCE:
        popsize: 500               # how many random samples for mpc
        max_iters: 8
        num_elites: 6
        minimal_elites: 3                 # threshold for minimal elites
        epsilon: 0.01 # for 0.003, it will has +- 0.05 error rate with 0.05 prob when converges
        alpha: 0.1 # weights for previous mean and var
        init_mean: 0
        init_var: 1

dynamic_config:
    n_ensembles: 4
    data_split: 0.8
    n_epochs: 70
    activation: relu
    batch_size: 256
    buffer_size: 500000
    hidden_sizes: [1024,1024,1024]
    learning_rate: 0.001
    test_freq: 5
    test_ratio: 0.15
    load: false
    load_folder: null
    save: false
    save_folder: null
